{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Iizuka_nb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyObAKG4GeF5pGbjVC3Y9MAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stmeinert/Recolorization_IANN/blob/main/Iizuka_nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open questions that are not described in the paper:\n",
        "\n",
        "\n",
        "*   How to transition from Conv2D- to Dense-Layer in Global Features Network?\n",
        "*   Where is BatchNormalization applied?\n",
        "*   What interpolation and cropping in Resizing-Layer?\n",
        "\n"
      ],
      "metadata": {
        "id": "B9B36DvptxIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "F5yWwWjeMnx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "KX2AHJsepQWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810d19ec-f0c0-493a-a89f-b1ee43fc872f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.7/dist-packages (0.24.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.24.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (0.24.0)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "!pip install tensorflow-io\n",
        "import tensorflow_io as tfio\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "brTGqVD_MlTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LowLevelFeatNet(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self): \n",
        "        super(LowLevelFeatNet, self).__init__()\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=156, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.net_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "T_DgeCJCxZWN"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MidLevelFeatNet(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self): \n",
        "        super(MidLevelFeatNet, self).__init__()\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.net_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TNuih3s6yw3z"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlobalFeatNet(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self): \n",
        "        super(GlobalFeatNet, self).__init__()\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        # NOTE: Paper does not specify how to transition from Conv2D- to Dense-Layer (Flatten causes number of variables to explode)\n",
        "        self.net_layers.append(tf.keras.layers.GlobalMaxPooling2D())\n",
        "        self.net_layers.append(tf.keras.layers.Dense(units=1024))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Dense(units=512))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Dense(units=256))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.net_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mIViRFHVy1Wp"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FusionLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self): \n",
        "        super(FusionLayer, self).__init__()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        \"\"\" Implementation of a similar approach can be found in https://github.com/baldassarreFe/deep-koalarization/blob/master/src/koalarization/fusion_layer.py \"\"\"\n",
        "        imgs, embs = x\n",
        "        reshaped_shape = imgs.shape[:3].concatenate(embs.shape[1])\n",
        "        embs = tf.repeat(embs, imgs.shape[1] * imgs.shape[2])\n",
        "        embs = tf.reshape(embs, reshaped_shape)\n",
        "        return tf.concat([imgs, embs], axis=3)"
      ],
      "metadata": {
        "id": "tkEhmu0RA3eQ"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationNet(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self): \n",
        "        super(ColorizationNet, self).__init__()\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(FusionLayer())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        self.net_layers.append(tf.keras.layers.UpSampling2D(size=(2,2), data_format='channels_last', interpolation='nearest'))\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        self.net_layers.append(tf.keras.layers.UpSampling2D(size=(2,2), data_format='channels_last', interpolation='nearest'))\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(2, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.sigmoid))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.net_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x"
      ],
      "metadata": {
        "id": "SS5R_0CDy-jl"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IizukaRecolorizationModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self): \n",
        "        super(IizukaRecolorizationModel, self).__init__()\n",
        "\n",
        "        self.rescale = tf.keras.layers.Resizing(224, 224, interpolation='nearest', crop_to_aspect_ratio=True)\n",
        "        self.low = LowLevelFeatNet()\n",
        "        self.mid = MidLevelFeatNet()\n",
        "        self.glob = GlobalFeatNet()\n",
        "        self.colorize = ColorizationNet()\n",
        "        self.upS = tf.keras.layers.UpSampling2D(size=(2,2), data_format='channels_last', interpolation='nearest')\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adadelta()\n",
        "        self.loss_function = tf.keras.losses.MeanSquaredError()\n",
        "        self.metrics_list = [\n",
        "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        # tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "                        # tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
        "                        ]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        re = self.rescale(x)\n",
        "        l1 = self.low(re)\n",
        "        g = self.glob(l1)\n",
        "\n",
        "        l2 = self.low(x)\n",
        "        m = self.mid(l2)\n",
        "\n",
        "        c = self.colorize((m,g))\n",
        "        return self.upS(c)\n",
        "\n",
        "    \n",
        "    def reset_metrics(self):\n",
        "        \n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "            \n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        \n",
        "        x, targets = data\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(x, training=True)\n",
        "            \n",
        "            loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update loss metric\n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        # for all metrics except loss, update states (accuracy etc.)\n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(targets,predictions)\n",
        "\n",
        "        # Return a dictionary mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "\n",
        "        x, targets = data\n",
        "        \n",
        "        predictions = self(x, training=False)\n",
        "        \n",
        "        loss = self.loss_function(targets, predictions) + tf.reduce_sum(self.losses)\n",
        "        \n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(targets, predictions)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "w9P2sczmzCtY"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Summary"
      ],
      "metadata": {
        "id": "cXCD8T3BMRA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing first model input\n",
        "myinput = tf.random.uniform(shape=(1,128,128,1), minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None, name=None)\n",
        "print(myinput)\n",
        "mymodel = IizukaRecolorizationModel()\n",
        "print(mymodel(myinput))\n",
        "print(mymodel.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvvMq7kr3TjQ",
        "outputId": "f082b194-e860-46af-d45b-ed3b22fd0bdc"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[0.0275408 ]\n",
            "   [0.46375656]\n",
            "   [0.1423949 ]\n",
            "   ...\n",
            "   [0.7775687 ]\n",
            "   [0.6604465 ]\n",
            "   [0.9971471 ]]\n",
            "\n",
            "  [[0.86306024]\n",
            "   [0.6645349 ]\n",
            "   [0.99020994]\n",
            "   ...\n",
            "   [0.07342672]\n",
            "   [0.5866457 ]\n",
            "   [0.52783   ]]\n",
            "\n",
            "  [[0.36038697]\n",
            "   [0.5252266 ]\n",
            "   [0.02579057]\n",
            "   ...\n",
            "   [0.64316726]\n",
            "   [0.12055314]\n",
            "   [0.8900324 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.1611799 ]\n",
            "   [0.7792616 ]\n",
            "   [0.42649996]\n",
            "   ...\n",
            "   [0.5582876 ]\n",
            "   [0.23931122]\n",
            "   [0.02001822]]\n",
            "\n",
            "  [[0.16111386]\n",
            "   [0.95230985]\n",
            "   [0.98579085]\n",
            "   ...\n",
            "   [0.42401528]\n",
            "   [0.82885313]\n",
            "   [0.7420373 ]]\n",
            "\n",
            "  [[0.05723846]\n",
            "   [0.3443383 ]\n",
            "   [0.21857095]\n",
            "   ...\n",
            "   [0.08773398]\n",
            "   [0.3194325 ]\n",
            "   [0.667907  ]]]], shape=(1, 128, 128, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[[0.4999587  0.50014246]\n",
            "   [0.4999587  0.50014246]\n",
            "   [0.49997485 0.5001073 ]\n",
            "   ...\n",
            "   [0.49987736 0.5000309 ]\n",
            "   [0.49999803 0.49996227]\n",
            "   [0.49999803 0.49996227]]\n",
            "\n",
            "  [[0.4999587  0.50014246]\n",
            "   [0.4999587  0.50014246]\n",
            "   [0.49997485 0.5001073 ]\n",
            "   ...\n",
            "   [0.49987736 0.5000309 ]\n",
            "   [0.49999803 0.49996227]\n",
            "   [0.49999803 0.49996227]]\n",
            "\n",
            "  [[0.49990454 0.50013304]\n",
            "   [0.49990454 0.50013304]\n",
            "   [0.49993718 0.5000506 ]\n",
            "   ...\n",
            "   [0.49994707 0.50003433]\n",
            "   [0.5000391  0.49994898]\n",
            "   [0.5000391  0.49994898]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.49997127 0.5001167 ]\n",
            "   [0.49997127 0.5001167 ]\n",
            "   [0.5000142  0.500207  ]\n",
            "   ...\n",
            "   [0.5001204  0.5000643 ]\n",
            "   [0.50015306 0.50000674]\n",
            "   [0.50015306 0.50000674]]\n",
            "\n",
            "  [[0.49997723 0.50006485]\n",
            "   [0.49997723 0.50006485]\n",
            "   [0.5000391  0.50007915]\n",
            "   ...\n",
            "   [0.5000957  0.50003815]\n",
            "   [0.50010854 0.5000227 ]\n",
            "   [0.50010854 0.5000227 ]]\n",
            "\n",
            "  [[0.49997723 0.50006485]\n",
            "   [0.49997723 0.50006485]\n",
            "   [0.5000391  0.50007915]\n",
            "   ...\n",
            "   [0.5000957  0.50003815]\n",
            "   [0.50010854 0.5000227 ]\n",
            "   [0.50010854 0.5000227 ]]]], shape=(1, 128, 128, 2), dtype=float32)\n",
            "Model: \"iizuka_recolorization_model_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resizing_24 (Resizing)      multiple                  0         \n",
            "                                                                 \n",
            " low_level_feat_net_24 (LowL  multiple                 1601164   \n",
            " evelFeatNet)                                                    \n",
            "                                                                 \n",
            " mid_level_feat_net_24 (MidL  multiple                 3542784   \n",
            " evelFeatNet)                                                    \n",
            "                                                                 \n",
            " global_feat_net_24 (GlobalF  multiple                 10636032  \n",
            " eatNet)                                                         \n",
            "                                                                 \n",
            " colorization_net_24 (Colori  multiple                 720866    \n",
            " zationNet)                                                      \n",
            "                                                                 \n",
            " up_sampling2d_74 (UpSamplin  multiple                 0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,500,848\n",
            "Trainable params: 16,488,566\n",
            "Non-trainable params: 12,282\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "OYoEXT-8MYA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SIZE = (128,128)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "#################################################\n",
        "# Prepare data\n",
        "#################################################\n",
        "\n",
        "@tf.function\n",
        "def resize(image):\n",
        "    return tf.image.resize_with_pad(image, target_height=SIZE[0], target_width=SIZE[1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def to_lab(image):\n",
        "    # expects input to be normalized to [0;1]!!\n",
        "    # output channels are [l,a,b]\n",
        "    return tfio.experimental.color.rgb_to_lab(image)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def to_grayscale(image):\n",
        "    # take l channel (size index starts at one^^)\n",
        "    image = tf.slice(image, begin=[0, 0, 0], size=[-1, -1, 1])\n",
        "    return image\n",
        "\n",
        "# @tf.function\n",
        "def prepare_image_data(image_ds):\n",
        "    # resize image to desired dimension, replace label with colored image\n",
        "    image_ds = image_ds.map(lambda x: (resize(x['image']), resize(x['image'])))\n",
        "\n",
        "    # normalize data to [0;1) for lab encoder\n",
        "    image_ds = image_ds.map(lambda image, target: ((image/256), (target/256)))\n",
        "\n",
        "    # convert image and target image to lab color space\n",
        "    image_ds = image_ds.map(lambda image, target: (to_lab(image), to_lab(target)))\n",
        "\n",
        "    # throw away L-dimension in target\n",
        "    # TODO: do slicing with tensorflow so that function can have decorator\n",
        "    image_ds = image_ds.map(lambda image, target: (image, target[:,:, :-2]))\n",
        "    # image_ds = image_ds.map(lambda image, target: (image, tf.slice(target, begin=[0, 0, 1], size=[-1, -1, 2])))\n",
        "\n",
        "    # only take l channel of input tensor\n",
        "    image_ds = image_ds.map(lambda image, target: (to_grayscale(image), target))\n",
        "\n",
        "    # l in lab is in [0;100] -> normalize to [0;1]/[-1;1]?\n",
        "    # ab are in range [-128;127]\n",
        "    image_ds = image_ds.map(lambda image, target: ((image/50)-1, target))\n",
        "\n",
        "    image_ds = image_ds.shuffle(1000).batch(BATCH_SIZE)#.prefetch(20)\n",
        "    return image_ds\n",
        "\n"
      ],
      "metadata": {
        "id": "b0NUarjC8K87"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboard Stuff"
      ],
      "metadata": {
        "id": "OCw_wlAZMbT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tensorboard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# to clear all logs use this line:\n",
        "!rm -rf ./logs/\n",
        "\n",
        "train_log_path = f\"logs/train\"\n",
        "val_log_path = f\"logs/val\"\n",
        "img_test_log_path = f\"logs/img_test\"\n",
        "\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "\n",
        "# log writer for validation metrics\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
        "\n",
        "img_test_summary_writer = tf.summary.create_file_writer(img_test_log_path)\n",
        "\n",
        "# show tensorboard\n",
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "Gb_YKix0818p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "7XqW4bUaMeMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TRAIN_IMAGES = ''\n",
        "TEST_IMAGES = '10'\n",
        "VAL_IMAGES = ''\n",
        "\n",
        "train_ds, val_ds = tfds.load(\"imagenette\", split=(f'train[:{TRAIN_IMAGES}]', f'validation[:{VAL_IMAGES}]'), as_supervised=False)\n",
        "\n",
        "train_ds = train_ds.apply(prepare_image_data)\n",
        "val_ds = val_ds.apply(prepare_image_data)\n",
        "\n",
        "\n",
        "model = IizukaRecolorizationModel()\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    print(f\"Epoch {epoch}:\")\n",
        "    start = time.time()\n",
        "\n",
        "    # Training:\n",
        "    \n",
        "    for data in tqdm.notebook.tqdm(train_ds,position=0, leave=True):\n",
        "        metrics = model.train_step(data)\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    # print the metrics\n",
        "    print(f\"Training took {end-start} seconds.\")\n",
        "    print([f\"{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
        "    \n",
        "    # logging the validation metrics to the log file which is used by tensorboard\n",
        "    with train_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "    \n",
        "    # reset all metrics (requires a reset_metrics method in the model)\n",
        "    model.reset_metrics()\n",
        "    \n",
        "    \n",
        "    # Validation:\n",
        "    \n",
        "    for data in tqdm.notebook.tqdm(val_ds,position=0, leave=True):\n",
        "        metrics = model.test_step(data)\n",
        "    \n",
        "    print([f\"val_{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
        "    \n",
        "    # logging the validation metrics to the log file which is used by tensorboard\n",
        "    with val_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
        "    \n",
        "    # reset all metrics\n",
        "    model.reset_metrics()\n",
        "\n",
        "    \n",
        "    # Test image:\n",
        "    sample = val_ds.take(1)\n",
        "    for input, target in sample:\n",
        "        prediction = model(input)\n",
        "        # get l channel, target should be in shape (SIZE, SIZE, lab)\n",
        "        l = tf.slice(target, begin=[0,0,0,0], size=[-1,-1,-1,1])\n",
        "        prediction = tf.concat([l, prediction], axis=-1) # should be concatenating along last dimension\n",
        "        prediction = tfio.experimental.color.lab_to_rgb(prediction)\n",
        "\n",
        "\n",
        "    with img_test_summary_writer.as_default():\n",
        "        tf.summary.image(name=\"generated_images\", step=epoch, data=prediction, max_outputs=5)\n",
        "\n",
        "\n",
        "    \n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "Jvvhl7R7Mcwf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}