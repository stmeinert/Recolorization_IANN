{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stmeinert/Recolorization_IANN/blob/main/Iizuka_nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9B36DvptxIT"
      },
      "source": [
        "Open questions that are not described in the paper:\n",
        "\n",
        "\n",
        "*   How to transition from Conv2D- to Dense-Layer in Global Features Network?\n",
        "*   Where is BatchNormalization applied?\n",
        "*   What interpolation and cropping in Resizing-Layer?\n",
        "*   What learning rate for AdaDelta?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9PxIaFZGpeD"
      },
      "source": [
        "# Util:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqzDPlu5Gfo0"
      },
      "outputs": [],
      "source": [
        "!rm -rf /drive/MyDrive/saved_model/model\n",
        "!rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5yWwWjeMnx3"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX2AHJsepQWs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "!pip install tensorflow-io\n",
        "import tensorflow_io as tfio\n",
        "import time\n",
        "import os \n",
        "import pickle\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brTGqVD_MlTz"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_DgeCJCxZWN"
      },
      "outputs": [],
      "source": [
        "class LowLevelFeatNet(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs): \n",
        "        super(LowLevelFeatNet, self).__init__(**kwargs)\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        \n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.net_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(LowLevelFeatNet, self).get_config()\n",
        "        # config.update({\n",
        "        #     \"net_layers\" : self.net_layers\n",
        "        # })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNuih3s6yw3z"
      },
      "outputs": [],
      "source": [
        "class MidLevelFeatNet(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs): \n",
        "        super(MidLevelFeatNet, self).__init__(**kwargs)\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.net_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(MidLevelFeatNet, self).get_config()\n",
        "        # config.update({\n",
        "        #     \"net_layers\" : self.net_layers\n",
        "        # })\n",
        "        return config\n",
        "        \n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIViRFHVy1Wp"
      },
      "outputs": [],
      "source": [
        "class GlobalFeatNet(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs): \n",
        "        super(GlobalFeatNet, self).__init__(**kwargs)\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(2,2), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(filters=512, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        # NOTE: Paper does not specify how to transition from Conv2D- to Dense-Layer (Flatten causes number of variables to explode)\n",
        "        self.net_layers.append(tf.keras.layers.GlobalMaxPooling2D())\n",
        "        self.net_layers.append(tf.keras.layers.Dense(units=1024))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Dense(units=512))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Dense(units=256))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.net_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(GlobalFeatNet, self).get_config()\n",
        "        # config.update({\n",
        "        #     \"net_layers\" : self.net_layers\n",
        "        # })\n",
        "        return config\n",
        "        \n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkEhmu0RA3eQ"
      },
      "outputs": [],
      "source": [
        "class FusionLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, batch_size, **kwargs): \n",
        "        super(FusionLayer, self).__init__(**kwargs)\n",
        "        self.batch_size = tf.constant(batch_size)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        \"\"\" Implementation of a similar approach can be found in https://github.com/baldassarreFe/deep-koalarization/blob/master/src/koalarization/fusion_layer.py \"\"\"\n",
        "        imgs, embs = x\n",
        "        reshaped_shape = tf.stack([tf.constant(self.batch_size), tf.constant(imgs.shape[1]), tf.constant(imgs.shape[2]), tf.constant(embs.shape[1])])\n",
        "        # reshaped_shape = imgs.shape[:3].concatenate(embs.shape[1])\n",
        "        embs = tf.repeat(embs, imgs.shape[1] * imgs.shape[2])\n",
        "        embs = tf.reshape(embs, reshaped_shape)\n",
        "        return tf.concat([imgs, embs], axis=3)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(FusionLayer, self).get_config()\n",
        "        # config.update({\n",
        "        #     \"batch_size\" : self.batch_size\n",
        "        # })\n",
        "        return config\n",
        "        \n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS5R_0CDy-jl"
      },
      "outputs": [],
      "source": [
        "class ColorizationNet(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, batch_size, **kwargs): \n",
        "        super(ColorizationNet, self).__init__(**kwargs)\n",
        "        self.net_layers = []\n",
        "        self.net_layers.append(FusionLayer(batch_size))\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(128, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        self.net_layers.append(tf.keras.layers.UpSampling2D(size=(2,2), data_format='channels_last', interpolation='nearest'))\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "        self.net_layers.append(tf.keras.layers.UpSampling2D(size=(2,2), data_format='channels_last', interpolation='nearest'))\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.relu))\n",
        "        self.net_layers.append(tf.keras.layers.BatchNormalization())\n",
        "        self.net_layers.append(tf.keras.layers.Conv2D(2, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "        self.net_layers.append(tf.keras.layers.Activation(tf.nn.sigmoid))\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        for layer in self.net_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ColorizationNet, self).get_config()\n",
        "        # config.update({\n",
        "        #     \"net_layers\" : self.net_layers\n",
        "        # })\n",
        "        return config\n",
        "        \n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9P2sczmzCtY"
      },
      "outputs": [],
      "source": [
        "class IizukaRecolorizationModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, batch_size, **kwargs): \n",
        "        super(IizukaRecolorizationModel, self).__init__(**kwargs)\n",
        "\n",
        "        self.rescale = tf.keras.layers.Resizing(224, 224, interpolation='nearest', crop_to_aspect_ratio=True)\n",
        "        self.low = LowLevelFeatNet()\n",
        "        self.mid = MidLevelFeatNet()\n",
        "        self.glob = GlobalFeatNet()\n",
        "        self.colorize = ColorizationNet(batch_size)\n",
        "        self.upS = tf.keras.layers.UpSampling2D(size=(2,2), data_format='channels_last', interpolation='nearest')\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adadelta(learning_rate=1.0)\n",
        "        self.loss_function = tf.keras.losses.MeanSquaredError()\n",
        "        self.metrics_list = [\n",
        "                        tf.keras.metrics.Mean(name=\"loss\"),\n",
        "                        # tf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\n",
        "                        # tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
        "                        ]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=False):\n",
        "        re = self.rescale(x, training=training)\n",
        "        l1 = self.low(re, training=training)\n",
        "        g = self.glob(l1, training=training)\n",
        "\n",
        "        l2 = self.low(x, training=training)\n",
        "        m = self.mid(l2, training=training)\n",
        "\n",
        "        c = self.colorize((m,g), training=training)\n",
        "        out = self.upS(c, training=training)\n",
        "\n",
        "        # bring the a-b-values from range [0,1] to [-128, 127]\n",
        "        out = out * 255.0\n",
        "        out = out - 128.0\n",
        "        return out\n",
        "\n",
        "    @tf.function\n",
        "    def reset_metrics(self):\n",
        "        \n",
        "        for metric in self.metrics:\n",
        "            metric.reset_states()\n",
        "            \n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        \n",
        "        x, targets = data\n",
        "\n",
        "        # throw away L-dimension in target\n",
        "        # TODO: do slicing with tensorflow so that function can have decorator\n",
        "        targets = targets[:,:,:,-2:]\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(x, training=True)\n",
        "            \n",
        "            loss = self.loss_function(targets, predictions)# + tf.reduce_sum(self.losses)\n",
        "        \n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update loss metric\n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        # for all metrics except loss, update states (accuracy etc.)\n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(targets,predictions)\n",
        "\n",
        "        # Return a dictionary mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(self, data):\n",
        "\n",
        "        x, targets = data\n",
        "\n",
        "        # throw away L-dimension in target\n",
        "        # TODO: do slicing with tensorflow so that function can have decorator\n",
        "        targets = targets[:,:,:,-2:]\n",
        "        \n",
        "        predictions = self(x, training=False)\n",
        "        \n",
        "        loss = self.loss_function(targets, predictions)# + tf.reduce_sum(self.losses)\n",
        "        \n",
        "        self.metrics[0].update_state(loss)\n",
        "        \n",
        "        for metric in self.metrics[1:]:\n",
        "            metric.update_state(targets, predictions)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(IizukaRecolorizationModel, self).get_config()\n",
        "        # config.update({\n",
        "        #     \"rescale\" : self.rescale,\n",
        "        #     \"low\" : self.low,\n",
        "        #     \"mid\" : self.mid,\n",
        "        #     \"glob\" : self.glob,\n",
        "        #     \"colorize\" : self.colorize,\n",
        "        #     \"uS\" : self.upS,\n",
        "        #     \"optimizer\" : self.optimizer,\n",
        "        #     \"loss_function\" : self.loss_function,\n",
        "        #     \"metrics_list\" : self.metrics_list\n",
        "        # })\n",
        "        return config\n",
        "        \n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXCD8T3BMRA6"
      },
      "source": [
        "# Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvvMq7kr3TjQ"
      },
      "outputs": [],
      "source": [
        "# testing first model input\n",
        "myinput = tf.random.uniform(shape=(2,128,128,1), minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None, name=None)\n",
        "mymodel = IizukaRecolorizationModel(2)\n",
        "mymodel(myinput)\n",
        "\n",
        "print(mymodel.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYoEXT-8MYA6"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0NUarjC8K87"
      },
      "outputs": [],
      "source": [
        "\n",
        "DS_NAME = \"celeb_data_set_preprocessed_part_0_3\"\n",
        "# DS_NAME = \"celeb_data_set_preprocessed_part_0_15_tiny_64_30000\"\n",
        "\n",
        "ZIP_DS_PATH = '/content/drive/MyDrive/' + DS_NAME + '.zip'\n",
        "EXTRACT_DS_PATH = '/content/current/Dataset'\n",
        "\n",
        "SIZE = (128,128)\n",
        "\n",
        "#################################################\n",
        "# Prepare data\n",
        "#################################################\n",
        "\n",
        "@tf.function\n",
        "def resize(image):\n",
        "    return tf.image.resize_with_pad(image, target_height=SIZE[0], target_width=SIZE[1], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def to_lab(image):\n",
        "    # expects input to be normalized to [0;1]!!\n",
        "    # output channels are [l,a,b]\n",
        "    return tfio.experimental.color.rgb_to_lab(image)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def to_grayscale(image):\n",
        "    # take l channel (size index starts at one^^)\n",
        "    image = tf.slice(image, begin=[0, 0, 0], size=[-1, -1, 1])\n",
        "    return image\n",
        "\n",
        "@tf.function\n",
        "def prepare_image_data(image_ds):\n",
        "    # resize image to desired dimension, replace label with colored image\n",
        "    image_ds = image_ds.map(lambda x: (resize(x['image']), resize(x['image'])))\n",
        "\n",
        "    # normalize data to [0;1) for lab encoder\n",
        "    image_ds = image_ds.map(lambda image, target: ((image/256), (target/256)))\n",
        "\n",
        "    # convert image and target image to lab color space\n",
        "    image_ds = image_ds.map(lambda image, target: (to_lab(image), to_lab(target)))\n",
        "\n",
        "    # only take l channel of input tensor\n",
        "    image_ds = image_ds.map(lambda image, target: (to_grayscale(image), target))\n",
        "\n",
        "    # l in lab is in [0;100] -> normalize to [0;1]/[-1;1]?\n",
        "    # ab are in range [-128;127]\n",
        "    image_ds = image_ds.map(lambda image, target: ((image/50)-1, target))\n",
        "\n",
        "    image_ds = image_ds.shuffle(1000).batch(BATCH_SIZE)#.prefetch(20)\n",
        "    return image_ds\n",
        "\n",
        "def prepare_validation_data(image_ds):\n",
        "    \"\"\"\n",
        "    Same as for train and test data, but don't shuffle so you can the progress over same image in tensorboard\n",
        "    \"\"\"\n",
        "    # resize image to desired dimension, replace label with colored image\n",
        "    image_ds = image_ds.map(lambda x: (resize(x['image']), resize(x['image'])))\n",
        "\n",
        "    # normalize data to [0;1) for lab encoder\n",
        "    image_ds = image_ds.map(lambda image, target: ((image/256), (target/256)))\n",
        "\n",
        "    # convert image and target image to lab color space\n",
        "    image_ds = image_ds.map(lambda image, target: (to_lab(image), to_lab(target)))\n",
        "\n",
        "    # only take l channel of input tensor\n",
        "    image_ds = image_ds.map(lambda image, target: (to_grayscale(image), target))\n",
        "\n",
        "    # l in lab is in [0;100] -> normalize to [-1;1]\n",
        "    # ab are in range [-128;127]\n",
        "    image_ds = image_ds.map(lambda image, target: ((image/50)-1, target))\n",
        "\n",
        "    image_ds = image_ds.batch(BATCH_SIZE).prefetch(20)\n",
        "    return image_ds\n",
        "\n",
        "def unzip_and_load_ds():\n",
        "    path = os.path.join(os.getcwd(), EXTRACT_DS_PATH, 'content', DS_NAME)\n",
        "\n",
        "    # only extract again if path does not exist!\n",
        "    if not os.path.exists(path):\n",
        "      with zipfile.ZipFile(ZIP_DS_PATH, 'r') as zip_ref:\n",
        "          zip_ref.extractall(EXTRACT_DS_PATH)\n",
        "\n",
        "    return tf.data.experimental.load(path,compression= 'GZIP')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboard"
      ],
      "metadata": {
        "id": "Z4tiV6OjVirD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tensorboard extension\n",
        "%load_ext tensorboard\n",
        "# show tensorboard\n",
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "b7uCX5jiA_7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XqW4bUaMeMT"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get Dataset in place\n",
        "\n",
        "# size of training, test and validation sets\n",
        "TRAIN_IMAGES = 500 * BATCH_SIZE\n",
        "TEST_IMAGES = 1 * BATCH_SIZE\n",
        "VAL_IMAGES = 50 * BATCH_SIZE\n",
        "TRAIN_IMAGES = 1 if (TRAIN_IMAGES // BATCH_SIZE) == 0 else (TRAIN_IMAGES // BATCH_SIZE)\n",
        "TEST_IMAGES = 1 if (TEST_IMAGES // BATCH_SIZE) == 0 else (TEST_IMAGES // BATCH_SIZE)\n",
        "VAL_IMAGES = 1 if (VAL_IMAGES // BATCH_SIZE) == 0 else (VAL_IMAGES // BATCH_SIZE)\n",
        "EPOCHS = 50\n",
        "\n",
        "ds = unzip_and_load_ds()\n",
        "test_ds = ds.take(TEST_IMAGES)\n",
        "val_ds = ds.skip(TEST_IMAGES).take(VAL_IMAGES)\n",
        "train_ds = ds.skip(TEST_IMAGES+VAL_IMAGES).take(TRAIN_IMAGES)"
      ],
      "metadata": {
        "id": "MDDqB4lvW8c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvvhl7R7Mcwf"
      },
      "outputs": [],
      "source": [
        "def load_model(model):\n",
        "    print(\"Load model...\")\n",
        "    \n",
        "    # # load optimizer weights\n",
        "    # with open(\"saved_model/opt_weights.npy\", \"rb\") as f:\n",
        "    #     loaded_weights = np.load(f, allow_pickle=True)\n",
        "    # grad_vars = model.trainable_weights\n",
        "    # zero_grads = [tf.zeros_like(w) for w in grad_vars]\n",
        "    # # Apply gradients which don't do nothing\n",
        "    # model.optimizer.apply_gradients(zip(zero_grads, grad_vars))\n",
        "    # # model.make_train_function()\n",
        "    # model.optimizer.set_weights(loaded_weights)\n",
        "\n",
        "    # load whole model\n",
        "    model = tf.keras.models.load_model(model_save_loc, \n",
        "                                       compile=False, \n",
        "                                       custom_objects={\n",
        "                                           \"IizukaRecolorizationModel\": IizukaRecolorizationModel, \n",
        "                                        #    \"ColorizationNet\" : ColorizationNet,\n",
        "                                        #    \"FusionLayer\" : FusionLayer,\n",
        "                                        #    \"GlobalFeatNet\" : GlobalFeatNet,\n",
        "                                        #    \"MidLevelFeatNet\" : MidLevelFeatNet,\n",
        "                                        #    \"LowLevelFeatNet\" : LowLevelFeatNet\n",
        "                                           }\n",
        "                                       )\n",
        "    # model = tf.saved_model.load(model_save_loc)   # doesn't seem to work\n",
        "    \n",
        "    # # load only weights\n",
        "    # model.load_weights(model_save_loc)\n",
        "\n",
        "    # load epoch number\n",
        "    with open(\"saved_model/epoch.dump\", \"rb\") as f:\n",
        "        epoch = pickle.load(f)\n",
        "\n",
        "    return model, epoch\n",
        "\n",
        "def save_model(model, epoch):\n",
        "    # save the model for this epoch\n",
        "    model.save(model_save_loc, save_format=\"tf\", save_traces=False)\n",
        "    # tf.saved_model.save(model, model_save_loc)    # doesn't seem to work\n",
        "\n",
        "    # # save only weights\n",
        "    # model.save_weights(model_save_loc, save_format='tf')\n",
        "\n",
        "    # save epoch number\n",
        "    with open(\"saved_model/epoch.dump\", \"wb\") as f:\n",
        "        pickle.dump(epoch, f)\n",
        "    \n",
        "    # # save optimizer weights\n",
        "    # weight_values = model.optimizer.get_weights()\n",
        "    # with open(\"saved_model/opt_weights.npy\", \"wb\") as f:\n",
        "    #     np.save(f, weight_values, allow_pickle=True)\n",
        "\n",
        "\n",
        "print(\"GPU in use:\")\n",
        "!nvidia-smi -L\n",
        "print(\"#############################\")\n",
        "\n",
        "\n",
        "# get the model in place\n",
        "model = IizukaRecolorizationModel(BATCH_SIZE)\n",
        "model_save_loc = \"/content/drive/MyDrive/checkpoints\"\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=model.optimizer, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, model_save_loc, max_to_keep=3)\n",
        "log_save_loc = \"./logs\"\n",
        "\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "if manager.latest_checkpoint:\n",
        "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "else:\n",
        "    print(\"Initializing from scratch.\")\n",
        "    #  clear all logs if the model is created newly and not loaded\n",
        "    !rm -rf ./logs/\n",
        "\n",
        "\n",
        "train_log_path = f\"{log_save_loc}/train\"\n",
        "val_log_path = f\"{log_save_loc}/val\"\n",
        "img_test_log_path = f\"{log_save_loc}/img_test\"\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "# log writer for validation metrics\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
        "# log writer for test images\n",
        "test_summary_writer = tf.summary.create_file_writer(img_test_log_path)\n",
        "\n",
        "\n",
        "# save first version validation images before training starts\n",
        "print(\"Getting first example images from untrained model\")\n",
        "for input, target in tqdm.notebook.tqdm(test_ds.take(1),position=0, leave=True):\n",
        "    prediction = model(input)\n",
        "    # get l channel, target should be in shape (SIZE, SIZE, lab)\n",
        "    l = tf.slice(target, begin=[0,0,0,0], size=[-1,-1,-1,1])\n",
        "    prediction = tf.concat([l, prediction], axis=-1) # should be concatenating along last dimension\n",
        "    prediction = tfio.experimental.color.lab_to_rgb(prediction)\n",
        "    target = tfio.experimental.color.lab_to_rgb(target)\n",
        "    input = (input+1)/2\n",
        "\n",
        "    with test_summary_writer.as_default():\n",
        "        tf.summary.image('Target', data=target, step=int(ckpt.step), max_outputs=16)\n",
        "        tf.summary.image(name=\"Prediction\", data=prediction, step=int(ckpt.step), max_outputs=16)\n",
        "        tf.summary.image(name=\"Input\", data=input, step=int(ckpt.step), max_outputs=16)\n",
        "\n",
        "while int(ckpt.step) < EPOCHS:\n",
        "    ckpt.step.assign_add(1)\n",
        "    print(f\"Epoch {int(ckpt.step)}:\")\n",
        "    start = time.time()\n",
        "\n",
        "    ### Training:\n",
        "    \n",
        "    for input, target in tqdm.notebook.tqdm(train_ds, position=0, leave=True):\n",
        "        metrics = model.train_step((input, target))\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    # print the metrics\n",
        "    print(f\"Training took {end-start} seconds.\")\n",
        "    print([f\"{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
        "    \n",
        "    # logging the validation metrics to the log file which is used by tensorboard\n",
        "    with train_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=int(ckpt.step))\n",
        "    \n",
        "    # reset all metrics (requires a reset_metrics method in the model)\n",
        "    model.reset_metrics()\n",
        "    \n",
        "    \n",
        "    ### Validation:\n",
        "    \n",
        "    for input, target in tqdm.notebook.tqdm(val_ds,position=0, leave=True):\n",
        "        metrics = model.test_step((input, target))\n",
        "    \n",
        "    print([f\"val_{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
        "    \n",
        "    # logging the validation metrics to the log file which is used by tensorboard\n",
        "    with val_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=int(ckpt.step))\n",
        "    \n",
        "    # reset all metrics\n",
        "    model.reset_metrics()\n",
        "\n",
        "    \n",
        "    ### Test image:\n",
        "\n",
        "    for input, target in tqdm.notebook.tqdm(test_ds.take(1),position=0, leave=True):\n",
        "        prediction = model(input)\n",
        "        \n",
        "        # get l channel, target should be in shape (SIZE, SIZE, lab)\n",
        "        l = tf.slice(target, begin=[0,0,0,0], size=[-1,-1,-1,1])\n",
        "        prediction = tf.concat([l, prediction], axis=-1) # should be concatenating along last dimension\n",
        "        prediction = tfio.experimental.color.lab_to_rgb(prediction)\n",
        "\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.image(name=\"Prediction\", data=prediction, step=int(ckpt.step), max_outputs=16)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    save_path = manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}: {}\".format(int(ckpt.step), save_path))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "brTGqVD_MlTz",
        "cXCD8T3BMRA6",
        "OYoEXT-8MYA6",
        "Z4tiV6OjVirD"
      ],
      "name": "Iizuka_nb.ipynb",
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "1RyVOj8V1EPg7KQ7LDxtdhBRYJhd8Xvlw",
      "authorship_tag": "ABX9TyO0237H9QtSrcFjlBOs2OA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}