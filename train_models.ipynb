{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "mount_file_id": "1qDrzOe1q-i8Bbfh_NeQLWLJJgP4PQWix",
      "authorship_tag": "ABX9TyNR5yCUeTHMkam0G2C6+PCW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stmeinert/Recolorization_IANN/blob/main/train_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports:"
      ],
      "metadata": {
        "id": "_HmuAKhyQwji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9HMRlXPTd5o"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/stmeinert/Recolorization_IANN.git\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tqdm\n",
        "!pip install tensorflow-io\n",
        "import tensorflow_io as tfio\n",
        "import time\n",
        "import os \n",
        "\n",
        "import sys\n",
        "if not \"/content/Recolorization_IANN\" in sys.path:\n",
        "    sys.path.append(\"/content/Recolorization_IANN\")\n",
        "from src.iizuka.iizuka_recolorization_model import IizukaRecolorizationModel\n",
        "\n",
        "from src.data_util.data_pipeline_util import unzip_and_load_ds\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter:"
      ],
      "metadata": {
        "id": "iNYda_8vRYLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "model = IizukaRecolorizationModel(BATCH_SIZE)\n",
        "\n",
        "# DS_NAME = \"celeb_data_set_preprocessed_part_0_3\"\n",
        "DS_NAME = \"celeb_data_set_unbatch_30000\"\n",
        "\n",
        "ZIP_DS_PATH = '/content/drive/MyDrive/' + DS_NAME + '.zip'\n",
        "EXTRACT_DS_PATH = '/content/current/Dataset'\n",
        "\n",
        "\n",
        "# size of training, test and validation sets\n",
        "TRAIN_IMAGES = 1000\n",
        "TEST_IMAGES = 100\n",
        "VAL_IMAGES = 50\n",
        "\n",
        "EPOCHS = 50\n",
        "\n",
        "\n",
        "MODEL_SAVE_LOCATION = \"/content/drive/MyDrive/checkpoints\"\n",
        "LOG_SAVE_LOCATION = \"./logs\""
      ],
      "metadata": {
        "id": "NY0XdMY1RccB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensorboard:"
      ],
      "metadata": {
        "id": "7_HMraxCQ1qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load tensorboard extension\n",
        "%load_ext tensorboard\n",
        "# show tensorboard\n",
        "%tensorboard --logdir $LOG_SAVE_LOCATION"
      ],
      "metadata": {
        "id": "_hr_aY6-Q4uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing:"
      ],
      "metadata": {
        "id": "dPTDR1bKQ6KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def prepare_train_dataset(image_ds):\n",
        "    return image_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "@tf.function\n",
        "def prepare_test_dataset(image_ds):\n",
        "    return image_ds.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "### get Dataset in place\n",
        "\n",
        "ds = unzip_and_load_ds(DS_NAME, EXTRACT_DS_PATH, ZIP_DS_PATH)\n",
        "train_ds = prepare_train_dataset(ds.take(TRAIN_IMAGES))\n",
        "test_ds = prepare_test_dataset(ds.skip(TRAIN_IMAGES).take(TEST_IMAGES))\n",
        "val_ds = prepare_train_dataset(ds.skip(TRAIN_IMAGES+TEST_IMAGES).take(VAL_IMAGES))"
      ],
      "metadata": {
        "id": "bzNrhzpZRHIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main:"
      ],
      "metadata": {
        "id": "Uw9du1x0XzEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"################ GPU in use: ################\")\n",
        "!nvidia-smi -L\n",
        "print(\"#############################################\")\n",
        "\n",
        "\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=model.optimizer, net=model)\n",
        "manager = tf.train.CheckpointManager(ckpt, MODEL_SAVE_LOCATION, max_to_keep=3)\n",
        "\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "if manager.latest_checkpoint:\n",
        "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "else:\n",
        "    print(\"Initializing from scratch.\")\n",
        "    !mkdir $MODEL_SAVE_LOCATION\n",
        "    #  clear all logs if the model is created newly and not loaded\n",
        "    !rm -rf $LOG_SAVE_LOCATION\n",
        "\n",
        "\n",
        "train_log_path = f\"{LOG_SAVE_LOCATION}/train\"\n",
        "val_log_path = f\"{LOG_SAVE_LOCATION}/val\"\n",
        "img_test_log_path = f\"{LOG_SAVE_LOCATION}/img_test\"\n",
        "# log writer for training metrics\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
        "# log writer for validation metrics\n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
        "# log writer for test images\n",
        "test_summary_writer = tf.summary.create_file_writer(img_test_log_path)\n",
        "\n",
        "# save first version validation images before training starts\n",
        "print(\"Getting first example images from untrained model\")\n",
        "for input, target in tqdm.notebook.tqdm(test_ds.take(1),position=0, leave=True):\n",
        "    prediction = model(input)\n",
        "    # get l channel, target should be in shape (SIZE, SIZE, lab)\n",
        "    l = tf.slice(target, begin=[0,0,0,0], size=[-1,-1,-1,1])\n",
        "    prediction = tf.concat([l, prediction], axis=-1) # should be concatenating along last dimension\n",
        "    prediction = tfio.experimental.color.lab_to_rgb(prediction)\n",
        "    target = tfio.experimental.color.lab_to_rgb(target)\n",
        "    input = (input+1)/2\n",
        "\n",
        "    with test_summary_writer.as_default():\n",
        "        tf.summary.image('Target', data=target, step=int(ckpt.step), max_outputs=16)\n",
        "        tf.summary.image(name=\"Prediction\", data=prediction, step=int(ckpt.step), max_outputs=16)\n",
        "        tf.summary.image(name=\"Input\", data=input, step=int(ckpt.step), max_outputs=16)\n",
        "\n",
        "while int(ckpt.step) < EPOCHS:\n",
        "    ckpt.step.assign_add(1)\n",
        "    print(f\"Epoch {int(ckpt.step)}:\")\n",
        "    start = time.time()\n",
        "\n",
        "    ### Training:\n",
        "    \n",
        "    for input, target in tqdm.notebook.tqdm(train_ds, position=0, leave=True):\n",
        "        metrics = model.train_step((input, target))\n",
        "\n",
        "    end = time.time()\n",
        "    \n",
        "    # print the metrics\n",
        "    print(f\"Training took {end-start} seconds.\")\n",
        "    print([f\"{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
        "    \n",
        "    # logging the validation metrics to the log file which is used by tensorboard\n",
        "    with train_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=int(ckpt.step))\n",
        "    \n",
        "    # reset all metrics (requires a reset_metrics method in the model)\n",
        "    model.reset_metrics()\n",
        "    \n",
        "    \n",
        "    ### Validation:\n",
        "    \n",
        "    for input, target in tqdm.notebook.tqdm(val_ds,position=0, leave=True):\n",
        "        metrics = model.test_step((input, target))\n",
        "    \n",
        "    print([f\"val_{key}: {value}\" for (key, value) in zip(list(metrics.keys()), list(metrics.values()))])\n",
        "    \n",
        "    # logging the validation metrics to the log file which is used by tensorboard\n",
        "    with val_summary_writer.as_default():\n",
        "        for metric in model.metrics:\n",
        "            tf.summary.scalar(f\"{metric.name}\", metric.result(), step=int(ckpt.step))\n",
        "    \n",
        "    # reset all metrics\n",
        "    model.reset_metrics()\n",
        "\n",
        "    \n",
        "    ### Test image:\n",
        "\n",
        "    for input, target in tqdm.notebook.tqdm(test_ds.take(1),position=0, leave=True):\n",
        "        prediction = model(input)\n",
        "        \n",
        "        # get l channel, target should be in shape (SIZE, SIZE, lab)\n",
        "        l = tf.slice(target, begin=[0,0,0,0], size=[-1,-1,-1,1])\n",
        "        prediction = tf.concat([l, prediction], axis=-1) # should be concatenating along last dimension\n",
        "        prediction = tfio.experimental.color.lab_to_rgb(prediction)\n",
        "\n",
        "        with test_summary_writer.as_default():\n",
        "            tf.summary.image(name=\"Prediction\", data=prediction, step=int(ckpt.step), max_outputs=16)\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    save_path = manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}: {}\".format(int(ckpt.step), save_path))"
      ],
      "metadata": {
        "id": "DOyRxpGXX0Rv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}